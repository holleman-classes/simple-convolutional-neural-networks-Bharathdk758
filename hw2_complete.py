# -*- coding: utf-8 -*-
"""hw2_complete.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15Mj0VEyZ2k0vcuSmufKDadpQ-ZoQg0Mb
"""

# Import required libraries
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import datasets, layers, models, Sequential
from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense, Dropout, Input, SeparableConv2D, Add
from tensorflow.keras.models import Model
import matplotlib.pyplot as plt
from PIL import Image
from google.colab import files

# Function to load and preprocess CIFAR10 dataset
def load_and_preprocess_cifar10():
    (x_train_full, y_train_full), (x_test, y_test) = datasets.cifar10.load_data()
    x_train, x_val = x_train_full[:45000] / 255.0, x_train_full[45000:] / 255.0
    y_train, y_val = y_train_full[:45000], y_train_full[45000:]
    x_test = x_test / 255.0

    # One-hot encode the labels
    y_train = tf.keras.utils.to_categorical(y_train, 10)
    y_val = tf.keras.utils.to_categorical(y_val, 10)
    y_test = tf.keras.utils.to_categorical(y_test, 10)
    return x_train, y_train, x_val, y_val, x_test, y_test

# First model architecture
def build_first_model():
    model = Sequential([
        Conv2D(32, (3, 3), strides=(2, 2), padding='same', activation='relu', input_shape=(32, 32, 3)),
        BatchNormalization(),
        Conv2D(64, (3, 3), strides=(2, 2), padding='same', activation='relu'),
        BatchNormalization(),
        Conv2D(128, (3, 3), strides=(2, 2), padding='same', activation='relu'),
        BatchNormalization(),
        *[Sequential([Conv2D(128, (3, 3), padding='same', activation='relu'), BatchNormalization()]) for _ in range(4)],
        MaxPooling2D((4, 4), strides=(4, 4)),
        Flatten(),
        Dense(128, activation='relu'),
        Dense(10, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Second model architecture with SeparableConv2D
def build_second_model():
    model = Sequential([
        Conv2D(32, (3, 3), strides=(2, 2), padding='same', activation='relu', input_shape=(32, 32, 3)),
        BatchNormalization(),
        SeparableConv2D(64, (3, 3), strides=(2, 2), padding='same', activation='relu'),
        BatchNormalization(),
        SeparableConv2D(128, (3, 3), strides=(2, 2), padding='same', activation='relu'),
        BatchNormalization(),
        *[Sequential([SeparableConv2D(128, (3, 3), padding='same', activation='relu'), BatchNormalization()]) for _ in range(4)],
        MaxPooling2D((4, 4), strides=(4, 4)),
        Flatten(),
        Dense(128, activation='relu'),
        Dense(10, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Third model architecture with functional API
def build_third_model():
    inputs = Input(shape=(32, 32, 3))
    x = Conv2D(32, (3, 3), strides=(2, 2), padding='same', activation='relu')(inputs)
    x = BatchNormalization()(x)
    x = Conv2D(64, (3, 3), strides=(2, 2), padding='same', activation='relu')(x)
    x = BatchNormalization()(x)
    x = Conv2D(128, (3, 3), strides=(2, 2), padding='same', activation='relu')(x)
    x = BatchNormalization()(x)
    for _ in range(4):
        x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)
        x = BatchNormalization()(x)
    x = MaxPooling2D((4, 4), strides=(4, 4))(x)
    x = Flatten()(x)
    x = Dense(128, activation='relu')(x)
    outputs = Dense(10, activation='softmax')(x)
    model = Model(inputs=inputs, outputs=outputs)
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

def build_model50k():
  model = models.Sequential()

  # Conv 2D: 32 filters, 2x2 kernel, stride=2 (in both x,y dimensions), "same" padding
  model.add(layers.Conv2D(32, (2, 2), strides=(2, 2), padding='same', input_shape=(32, 32, 3)))
  model.add(layers.BatchNormalization())
  model.add(layers.Activation('relu'))

  model.add(layers.SeparableConv2D(64, kernel_size=(3, 3), activation="relu", strides=(2, 2), padding='same'))
  model.add(layers.BatchNormalization())
  model.add(layers.SeparableConv2D(128, kernel_size=(2, 2), activation="relu", strides=(2, 2), padding='same'))
  model.add(layers.BatchNormalization())

  model.add(layers.Dropout(0.2))

  model.add(layers.SeparableConv2D(128, kernel_size=(3, 3), activation="relu", padding='same'))
  model.add(layers.BatchNormalization())

  model.add(layers.Dropout(0.2))

  model.add(layers.MaxPooling2D(pool_size=(4, 4), strides=(4, 4), padding='same'))

  model.add(layers.Flatten())

  model.add(layers.Dense(128, activation='relu'))
  model.add(layers.Dropout(0.2))

  model.add(layers.Dense(10, activation='softmax'))

  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

  model.summary()

  return model


# no training or dataset construction should happen above this line
if __name__ == '__main__':

    # Load CIFAR-10 dataset
    x_train, y_train, x_val, y_val, x_test, y_test = load_and_preprocess_cifar10()

# Build and train the first model
model1 = build_first_model()
model1.summary()
history = model1.fit(x_train, y_train, epochs=50, batch_size=128, validation_data=(x_val, y_val))
model1.evaluate(x_test, y_test, verbose=2)
# Plotting the training and validation loss
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(loc='upper right')

# Plotting the training and validation accuracy
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(loc='lower right')

plt.tight_layout()
plt.show()

# Build and train the first model
model2 = build_second_model()
model2.summary()
history = model2.fit(x_train, y_train, epochs=50, batch_size=128, validation_data=(x_val, y_val))
model2.evaluate(x_test, y_test, verbose=2)
# Plotting the training and validation loss
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(loc='upper right')

# Plotting the training and validation accuracy
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(loc='lower right')

plt.tight_layout()
plt.show()

# Build and train the first model
model3 = build_third_model()
model3.summary()
history = model3.fit(x_train, y_train, epochs=50, batch_size=128, validation_data=(x_val, y_val))
model3.evaluate(x_test, y_test, verbose=2)
# Plotting the training and validation loss
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(loc='upper right')

# Plotting the training and validation accuracy
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(loc='lower right')

plt.tight_layout()
plt.show()

# Build and train the first model
model50k = build_model50k()
model50k.summary()
history = model50k.fit(x_train, y_train, epochs=50, batch_size=128, validation_data=(x_val, y_val))
model50k.evaluate(x_test, y_test, verbose=2)
# Plotting the training and validation loss
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(loc='upper right')

# Plotting the training and validation accuracy
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(loc='lower right')

plt.tight_layout()
plt.show()

#Save the trained model
  model1.save('model1.h5')
  model2.save('model2.h5')
  model3.save('model3.h5')
  model50k.save('best_model.h5')

uploaded = files.upload()

# Define class names for CIFAR-10 dataset
class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']

def predict_image_class(image_path, model_path):
    # Load and resize the image
    img = Image.open(image_path)
    img = img.resize((32, 32))

    # Convert the image to a numpy array and add a batch dimension
    img_array = np.array(img)
    img_array = img_array.reshape((1,) + img_array.shape)  # Add batch dimension

    # Load the trained model
    model = keras.models.load_model(model_path)

    # Make a prediction on the image
    predictions = model.predict(img_array)

    # Get the predicted class label
    class_idx = np.argmax(predictions)
    class_name = class_names[class_idx]

    return class_name

# Predict and print the class label for the same image using different models
image_path = "cat.jpeg"
model_paths = ["model1.h5", "model2.h5", "model3.h5", "best_model.h5"]

for model_path in model_paths:
    class_label = predict_image_class(image_path, model_path)
    print(f"Predicted class label using {model_path}: {class_label}")

